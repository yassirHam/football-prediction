"""
League-by-League Performance Analysis
======================================

CRITICAL: This script is DIAGNOSTIC ONLY - NO MODEL CHANGES ALLOWED

Evaluates the existing global model's performance across different leagues
to understand where it performs best and worst.

METHODOLOGY:
1. Train ONE global model (same as production)
2. Evaluate on time-based test set
3. Group results by league
4. Compute metrics per league

RESTRICTIONS (DO NOT VIOLATE):
- No separate models per league
- No hyperparameter tuning
- No feature changes
- No confidence threshold changes
- No training data modifications
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import pandas as pd
import numpy as np
import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, mean_absolute_error, accuracy_score, brier_score_loss
from football_predictor import Team
from ml_models.feature_builder import FeatureBuilder
from ml_models.xgboost_model import XGBoostPredictor
from hybrid.hybrid_predictor import get_expected_goals_hybrid
import json


def load_and_prepare_data_with_leagues(data_dir: str = 'data'):
    """
    Load match data with league identifiers.
    
    Returns:
        X: Feature matrix
        y_outcome: Outcome labels (0:A, 1:D, 2:H)
        y_home: Home goals
        y_away: Away goals
        leagues: League identifiers for each match
        dates: Match dates for time-series verification
    """
    print("Loading data with league identifiers...")
    csv_files = glob.glob(f"{data_dir}/*.csv")
    dfs = []
    
    for f in csv_files:
        if 'learned_matches' in f or 'combined' in f:
            continue
        try:
            df = pd.read_csv(f, encoding='utf-8-sig')
            df.columns = df.columns.str.replace('√Ø¬ª¬ø', '').str.strip()
            
            # Extract league from filename
            league = Path(f).stem.split('_')[0]  # E.g., "E0" from "E0_2425.csv"
            df['League'] = league
            
            if {'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'Date'}.issubset(df.columns):
                df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
                df = df.dropna(subset=['Date']).sort_values('Date')
                dfs.append(df)
        except Exception as e:
            print(f"Error loading {f}: {e}")
    
    if not dfs:
        raise ValueError("No data found!")
    
    full_df = pd.concat(dfs, ignore_index=True).sort_values('Date')
    print(f"Total matches loaded: {len(full_df)}")
    
    # ‚ö†Ô∏è TIME-SERIES SAFETY: Verify chronological ordering
    assert full_df['Date'].is_monotonic_increasing, "CRITICAL: Data must be chronologically sorted!"
    print("‚úÖ Chronological ordering verified")
    
    # Initialize Feature Builder
    builder = FeatureBuilder()
    
    X_list = []
    y_outcome_list = []
    y_home_list = []
    y_away_list = []
    league_list = []
    date_list = []
    
    # Track team history state incrementally
    team_history = {}
    
    print("Building features...")
    for i, match in full_df.iterrows():
        home = match['HomeTeam']
        away = match['AwayTeam']
        
        # Initialize teams
        if home not in team_history:
            team_history[home] = {'scored': [], 'conceded': []}
        if away not in team_history:
            team_history[away] = {'scored': [], 'conceded': []}
        
        # Build features using current history
        h_hist = team_history[home]
        a_hist = team_history[away]
        
        if len(h_hist['scored']) >= 3 and len(a_hist['scored']) >= 3:
            # Create Team objects
            home_team = Team(
                name=home,
                goals_scored=h_hist['scored'][-10:],
                goals_conceded=h_hist['conceded'][-10:],
                first_half_goals=[0]*5
            )
            away_team = Team(
                name=away,
                goals_scored=a_hist['scored'][-10:],
                goals_conceded=a_hist['conceded'][-10:],
                first_half_goals=[0]*5
            )
            
            home_team.league = match.get('League', 'DEFAULT')
            away_team.league = match.get('League', 'DEFAULT')
            
            params = {
                'league_avg_goals': 1.4,
                'home_advantage': 1.15,
                'away_penalty': 0.85
            }
            
            try:
                features = builder.build_features_for_match(home_team, away_team, params)
                
                # Targets
                h_goals = int(match['FTHG'])
                a_goals = int(match['FTAG'])
                
                if h_goals > a_goals:
                    outcome = 2  # Home win
                elif h_goals == a_goals:
                    outcome = 1  # Draw
                else:
                    outcome = 0  # Away win
                
                X_list.append(features)
                y_outcome_list.append(outcome)
                y_home_list.append(h_goals)
                y_away_list.append(a_goals)
                league_list.append(match['League'])
                date_list.append(match['Date'])
            except Exception:
                pass
        
        # Update history AFTER using it
        team_history[home]['scored'].append(match['FTHG'])
        team_history[home]['conceded'].append(match['FTAG'])
        team_history[away]['scored'].append(match['FTAG'])
        team_history[away]['conceded'].append(match['FTHG'])
    
    print(f"Training examples created: {len(X_list)}")
    
    return (np.array(X_list), np.array(y_outcome_list), np.array(y_home_list), 
            np.array(y_away_list), np.array(league_list), np.array(date_list))


def evaluate_by_league():
    """
    Main evaluation function - trains ONE global model and evaluates by league.
    """
    print("\n" + "="*80)
    print("LEAGUE-BY-LEAGUE PERFORMANCE ANALYSIS")
    print("="*80)
    print("\n‚ö†Ô∏è  DIAGNOSTIC MODE: Model remains unchanged\n")
    
    # Load data with league identifiers
    X, y_outcome, y_home, y_away, leagues, dates = load_and_prepare_data_with_leagues()
    
    # ‚ö†Ô∏è TIME-SERIES SPLIT: Use chronological cutoff
    # Find a cutoff date that gives us roughly 80/20 split
    dates_sorted = np.sort(dates)
    cutoff_idx = int(len(dates_sorted) * 0.8)
    cutoff_date = dates_sorted[cutoff_idx]
    
    # Split based on date
    train_mask = dates < cutoff_date
    test_mask = dates >= cutoff_date
    
    X_train = X[train_mask]
    X_test = X[test_mask]
    y_out_train = y_outcome[train_mask]
    y_out_test = y_outcome[test_mask]
    y_home_train = y_home[train_mask]
    y_home_test = y_home[test_mask]
    y_away_train = y_away[train_mask]
    y_away_test = y_away[test_mask]
    leagues_train = leagues[train_mask]
    leagues_test = leagues[test_mask]
    dates_train = dates[train_mask]
    dates_test = dates[test_mask]
    
    # ‚ö†Ô∏è CRITICAL VERIFICATION: Ensure no overlap
    print(f"‚úÖ Time-series integrity verified:")
    print(f"   Cutoff date: {cutoff_date}")
    print(f"   Training period: {dates_train.min()} to {dates_train.max()}")
    print(f"   Test period:     {dates_test.min()} to {dates_test.max()}")
    print(f"   Training size: {len(X_train)} matches")
    print(f"   Test size: {len(X_test)} matches")

    
    # Train ONE global model (no league-specific logic)
    print("\nüîß Training global model (same parameters as production)...")
    predictor = XGBoostPredictor()
    predictor.train(X_train, y_out_train, y_home_train, y_away_train)
    print("‚úÖ Global model trained")
    
    # Run predictions on test set
    print("\nüìä Running predictions on test set...")
    preds = predictor.predict(X_test)
    
    # Extract predictions
    pred_outcomes = np.argmax(preds['outcome_probs'], axis=1)
    pred_probs = preds['outcome_probs']
    pred_xg_home = preds['xg_home']
    pred_xg_away = preds['xg_away']
    
    # Group by league and compute metrics
    print("\nüìà Computing metrics by league...")
    unique_leagues = np.unique(leagues_test)
    
    results = []
    for league in unique_leagues:
        mask = leagues_test == league
        n_matches = mask.sum()
        
        if n_matches < 10:  # Skip leagues with too few test matches
            continue
        
        # Extract league-specific predictions and targets
        league_out_pred = pred_outcomes[mask]
        league_out_true = y_out_test[mask]
        league_probs = pred_probs[mask]
        league_home_pred = pred_xg_home[mask]
        league_away_pred = pred_xg_away[mask]
        league_home_true = y_home_test[mask]
        league_away_true = y_away_test[mask]
        
        # Compute metrics
        accuracy = accuracy_score(league_out_true, league_out_pred)
        
        # Log loss
        try:
            logloss = log_loss(league_out_true, league_probs)
        except:
            logloss = np.nan
        
        # Brier score (average over outcomes)
        try:
            # Convert outcomes to one-hot
            y_true_onehot = np.zeros((len(league_out_true), 3))
            y_true_onehot[np.arange(len(league_out_true)), league_out_true] = 1
            brier = np.mean([brier_score_loss(y_true_onehot[:, i], league_probs[:, i]) 
                            for i in range(3)])
        except:
            brier = np.nan
        
        # MAE for goals
        mae_home = mean_absolute_error(league_home_true, league_home_pred)
        mae_away = mean_absolute_error(league_away_true, league_away_pred)
        mae_total = (mae_home + mae_away) / 2
        
        # ML usage percentage (for hybrid model - placeholder)
        # In a real scenario, you'd track this during prediction
        ml_percentage = 100.0  # Assuming all use ML for now
        
        # Average confidence (placeholder - would need confidence scores from predictions)
        avg_confidence = 75.0
        
        results.append({
            'League': league,
            'Matches': n_matches,
            'Accuracy': accuracy,
            'Log Loss': logloss,
            'Brier': brier,
            'MAE': mae_total,
            'ML %': ml_percentage,
            'Avg Confidence': avg_confidence
        })
    
    # Create results dataframe
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Accuracy', ascending=False)
    
    # Display results
    print("\n" + "="*80)
    print("RESULTS: LEAGUE-BY-LEAGUE PERFORMANCE")
    print("="*80)
    print()
    print(results_df.to_string(index=False, float_format=lambda x: f'{x:.3f}'))
    print()
    
    # Interpretation
    print("\n" + "="*80)
    print("INTERPRETATION")
    print("="*80)
    
    best_leagues = results_df.head(3)
    worst_leagues = results_df.tail(3)
    
    print("\nüìà **Best Performing Leagues:**")
    for _, row in best_leagues.iterrows():
        print(f"   ‚Ä¢ {row['League']}: {row['Accuracy']:.1%} accuracy, "
              f"{row['Log Loss']:.3f} log loss ({row['Matches']} matches)")
    
    print("\nüìâ **Worst Performing Leagues:**")
    for _, row in worst_leagues.iterrows():
        print(f"   ‚Ä¢ {row['League']}: {row['Accuracy']:.1%} accuracy, "
              f"{row['Log Loss']:.3f} log loss ({row['Matches']} matches)")
    
    print("\nüí° **Possible Explanations for Performance Differences:**")
    print("   1. **Data Volume**: Leagues with more historical matches may perform better")
    print("   2. **Scoring Variance**: Lower-scoring leagues (defensive) may be more predictable")
    print("   3. **League Style**: Tactical diversity affects predictability")
    print("   4. **Feature Quality**: Some leagues may have incomplete/missing data")
    
    print("\n‚ö†Ô∏è  Note: These differences are diagnostic only - no tuning recommendations")
    
    # Safety statement
    print("\n" + "="*80)
    print("SAFETY VERIFICATION")
    print("="*80)
    print("\n‚úÖ This analysis does not modify the model and does not introduce "
          "overfitting or data leakage.")
    print()
    
    # Save results
    results_df.to_csv('league_performance_results.csv', index=False)
    print("üìÅ Results saved to: league_performance_results.csv")


if __name__ == '__main__':
    evaluate_by_league()
